{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from janome.tokenizer import Tokenizer\n",
    "import re\n",
    "\n",
    "\n",
    "def wakati_reading(text):\n",
    "    j_tokenizer = Tokenizer()\n",
    "    tokens = j_tokenizer.tokenize(text.replace(\"'\", \"\").lower())\n",
    "    \n",
    "    exclude_pos = [u'助動詞']\n",
    "    \n",
    "    #分かち書き\n",
    "    tokens_w_space = \"\"\n",
    "    for token in tokens:\n",
    "        partOfSpeech = token.part_of_speech.split(',')[0]\n",
    "        \n",
    "        if partOfSpeech not in exclude_pos:\n",
    "            tokens_w_space = tokens_w_space + \" \" + token.surface\n",
    "\n",
    "    tokens_w_space = tokens_w_space.strip()\n",
    "    \n",
    "    #読み方\n",
    "    tokens_reading = \"\"\n",
    "    for token in tokens:\n",
    "        partOfSpeech = token.part_of_speech.split(',')[0]\n",
    " \n",
    "        if partOfSpeech not in exclude_pos:\n",
    "            if token.reading != \"*\":\n",
    "                tokens_reading = tokens_reading + \" \" + token.reading\n",
    "            elif re.match('^[a-z]+$', token.base_form):\n",
    "                alpha_reading = \"\"\n",
    "                alpha_reading = token.base_form.replace(\"a\", \"エー \")\n",
    "                alpha_reading = alpha_reading.replace(\"b\", \"ビー \")\n",
    "                alpha_reading = alpha_reading.replace(\"c\", \"シー \")\n",
    "                alpha_reading = alpha_reading.replace(\"d\", \"ディー \")\n",
    "                alpha_reading = alpha_reading.replace(\"e\", \"イー \")\n",
    "                alpha_reading = alpha_reading.replace(\"f\", \"エフ \")\n",
    "                alpha_reading = alpha_reading.replace(\"g\", \"ジー \")\n",
    "                alpha_reading = alpha_reading.replace(\"h\", \"エイチ \")\n",
    "                alpha_reading = alpha_reading.replace(\"i\", \"アイ \")\n",
    "                alpha_reading = alpha_reading.replace(\"j\", \"ジェー \")\n",
    "                alpha_reading = alpha_reading.replace(\"k\", \"ケー \")\n",
    "                alpha_reading = alpha_reading.replace(\"l\", \"エル \")\n",
    "                alpha_reading = alpha_reading.replace(\"m\", \"エム \")\n",
    "                alpha_reading = alpha_reading.replace(\"n\", \"エヌ \")\n",
    "                alpha_reading = alpha_reading.replace(\"o\", \"オー \")\n",
    "                alpha_reading = alpha_reading.replace(\"p\", \"ピー \")\n",
    "                alpha_reading = alpha_reading.replace(\"q\", \"キュー \")\n",
    "                alpha_reading = alpha_reading.replace(\"r\", \"アール \")\n",
    "                alpha_reading = alpha_reading.replace(\"s\", \"エス \")\n",
    "                alpha_reading = alpha_reading.replace(\"t\", \"ティー \")\n",
    "                alpha_reading = alpha_reading.replace(\"u\", \"ユー \")\n",
    "                alpha_reading = alpha_reading.replace(\"v\", \"ブイ \")\n",
    "                alpha_reading = alpha_reading.replace(\"w\", \"ダブリュー \")\n",
    "                alpha_reading = alpha_reading.replace(\"x\", \"エックス \")\n",
    "                alpha_reading = alpha_reading.replace(\"y\", \"ワイ \")\n",
    "                alpha_reading = alpha_reading.replace(\"z\", \"ゼット \")\n",
    "\n",
    "                tokens_reading = tokens_reading + \" \" + alpha_reading\n",
    "            elif re.match('^[0-9]+$', token.base_form):\n",
    "                numeric_reading = \"\"\n",
    "                numeric_reading = token.base_form.replace(\"0\", \"ゼロ \")\n",
    "                numeric_reading = numeric_reading.replace(\"1\", \"イチ \")\n",
    "                numeric_reading = numeric_reading.replace(\"2\", \"ニ \")\n",
    "                numeric_reading = numeric_reading.replace(\"3\", \"サン \")\n",
    "                numeric_reading = numeric_reading.replace(\"4\", \"ヨン \")\n",
    "                numeric_reading = numeric_reading.replace(\"5\", \"ゴ \")\n",
    "                numeric_reading = numeric_reading.replace(\"6\", \"ロク \")\n",
    "                numeric_reading = numeric_reading.replace(\"7\", \"ナナ \")\n",
    "                numeric_reading = numeric_reading.replace(\"8\", \"ハチ \")\n",
    "                numeric_reading = numeric_reading.replace(\"9\", \"キュー \")\n",
    "\n",
    "                tokens_reading = tokens_reading + \" \" + numeric_reading.strip()\n",
    "\n",
    "    tokens_reading = tokens_reading.strip()\n",
    "    \n",
    "    feature = tokens_w_space + \" \" + tokens_reading\n",
    "    \n",
    "    return feature\n",
    "\n",
    "def main():\n",
    "    data = pd.read_csv('./data/intent_data_jp_ja.csv', sep=',', names=['text', 'intent'])\n",
    "    le = preprocessing.LabelEncoder()\n",
    "\n",
    "    data['label'] = le.fit_transform(data['intent'])\n",
    "    \n",
    "    data = data.drop(['intent'], axis=1)\n",
    "    \n",
    "    data['feature'] = data['text'].apply(lambda x: wakati_reading(x))\n",
    "    data = data.drop(['text'], axis=1)\n",
    "    \n",
    "    train = data.drop(['label'], axis=1)\n",
    "    train_label = data['label']\n",
    "    \n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(train, train_label,\n",
    "                                                  test_size = .2,\n",
    "                                                  random_state=12)\n",
    "    \n",
    "    tf = TfidfVectorizer(analyzer='word', ngram_range=(1,6), max_features=10000,\n",
    "                    sublinear_tf=True, token_pattern=u'[A-Za-z0-9\\-ぁ-ヶ亜-黑ー]{1,}')\n",
    "\n",
    "    train_X_tf =  tf.fit_transform(train_X['feature'])\n",
    "    val_X_tf =  tf.transform(val_X['feature'])\n",
    "\n",
    "    clf_final = Pipeline([('tfidf', TfidfVectorizer(analyzer='word', ngram_range=(1,3), max_features=5000,\n",
    "                    sublinear_tf=True, token_pattern=u'[A-Za-z0-9\\-ぁ-ヶ亜-黑ー]{1,}')),\n",
    "                           ('clf', GradientBoostingClassifier(\n",
    "                                random_state = 1337,\n",
    "                                verbose = 0,\n",
    "                                n_estimators = 20,\n",
    "                                learning_rate = 0.1,\n",
    "                                loss = 'deviance',\n",
    "                                max_depth = 3\n",
    "                               ))])\n",
    "    \n",
    "    clf_final = clf_final.fit(train_X['feature'], train_Y)\n",
    "    \n",
    "    joblib.dump(clf_final, './model/ja_jp_v6.pkl')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from janome.tokenizer import Tokenizer\n",
    "import re\n",
    "\n",
    "def wakati_reading(text):\n",
    "    j_tokenizer = Tokenizer()\n",
    "    tokens = j_tokenizer.tokenize(text.replace(\"'\", \"\").lower())\n",
    "    \n",
    "    exclude_pos = [u'助動詞']\n",
    "    \n",
    "    #分かち書き\n",
    "    tokens_w_space = \"\"\n",
    "    for token in tokens:\n",
    "        partOfSpeech = token.part_of_speech.split(',')[0]\n",
    "        \n",
    "        if partOfSpeech not in exclude_pos:\n",
    "            tokens_w_space = tokens_w_space + \" \" + token.surface\n",
    "\n",
    "    tokens_w_space = tokens_w_space.strip()\n",
    "    \n",
    "    #読み方\n",
    "    tokens_reading = \"\"\n",
    "    for token in tokens:\n",
    "        partOfSpeech = token.part_of_speech.split(',')[0]\n",
    " \n",
    "        if partOfSpeech not in exclude_pos:\n",
    "            if token.reading != \"*\":\n",
    "                tokens_reading = tokens_reading + \" \" + token.reading\n",
    "            elif re.match('^[a-z]+$', token.base_form):\n",
    "                alpha_reading = \"\"\n",
    "                alpha_reading = token.base_form.replace(\"a\", \"エー \")\n",
    "                alpha_reading = alpha_reading.replace(\"b\", \"ビー \")\n",
    "                alpha_reading = alpha_reading.replace(\"c\", \"シー \")\n",
    "                alpha_reading = alpha_reading.replace(\"d\", \"ディー \")\n",
    "                alpha_reading = alpha_reading.replace(\"e\", \"イー \")\n",
    "                alpha_reading = alpha_reading.replace(\"f\", \"エフ \")\n",
    "                alpha_reading = alpha_reading.replace(\"g\", \"ジー \")\n",
    "                alpha_reading = alpha_reading.replace(\"h\", \"エイチ \")\n",
    "                alpha_reading = alpha_reading.replace(\"i\", \"アイ \")\n",
    "                alpha_reading = alpha_reading.replace(\"j\", \"ジェー \")\n",
    "                alpha_reading = alpha_reading.replace(\"k\", \"ケー \")\n",
    "                alpha_reading = alpha_reading.replace(\"l\", \"エル \")\n",
    "                alpha_reading = alpha_reading.replace(\"m\", \"エム \")\n",
    "                alpha_reading = alpha_reading.replace(\"n\", \"エヌ \")\n",
    "                alpha_reading = alpha_reading.replace(\"o\", \"オー \")\n",
    "                alpha_reading = alpha_reading.replace(\"p\", \"ピー \")\n",
    "                alpha_reading = alpha_reading.replace(\"q\", \"キュー \")\n",
    "                alpha_reading = alpha_reading.replace(\"r\", \"アール \")\n",
    "                alpha_reading = alpha_reading.replace(\"s\", \"エス \")\n",
    "                alpha_reading = alpha_reading.replace(\"t\", \"ティー \")\n",
    "                alpha_reading = alpha_reading.replace(\"u\", \"ユー \")\n",
    "                alpha_reading = alpha_reading.replace(\"v\", \"ブイ \")\n",
    "                alpha_reading = alpha_reading.replace(\"w\", \"ダブリュー \")\n",
    "                alpha_reading = alpha_reading.replace(\"x\", \"エックス \")\n",
    "                alpha_reading = alpha_reading.replace(\"y\", \"ワイ \")\n",
    "                alpha_reading = alpha_reading.replace(\"z\", \"ゼット \")\n",
    "\n",
    "                tokens_reading = tokens_reading + \" \" + alpha_reading\n",
    "            elif re.match('^[0-9]+$', token.base_form):\n",
    "                numeric_reading = \"\"\n",
    "                numeric_reading = token.base_form.replace(\"0\", \"ゼロ \")\n",
    "                numeric_reading = numeric_reading.replace(\"1\", \"イチ \")\n",
    "                numeric_reading = numeric_reading.replace(\"2\", \"ニ \")\n",
    "                numeric_reading = numeric_reading.replace(\"3\", \"サン \")\n",
    "                numeric_reading = numeric_reading.replace(\"4\", \"ヨン \")\n",
    "                numeric_reading = numeric_reading.replace(\"5\", \"ゴ \")\n",
    "                numeric_reading = numeric_reading.replace(\"6\", \"ロク \")\n",
    "                numeric_reading = numeric_reading.replace(\"7\", \"ナナ \")\n",
    "                numeric_reading = numeric_reading.replace(\"8\", \"ハチ \")\n",
    "                numeric_reading = numeric_reading.replace(\"9\", \"キュー \")\n",
    "\n",
    "                tokens_reading = tokens_reading + \" \" + numeric_reading.strip()\n",
    "\n",
    "    tokens_reading = tokens_reading.strip()\n",
    "    \n",
    "    feature = tokens_w_space + \" \" + tokens_reading\n",
    "    \n",
    "    return feature\n",
    "\n",
    "data = pd.read_csv('./data/intent_data_jp_ja.csv', sep=',', names=['text', 'intent'])\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "data['label'] = le.fit_transform(data['intent'])\n",
    "    \n",
    "data = data.drop(['intent'], axis=1)\n",
    "    \n",
    "data['feature'] = data['text'].apply(lambda x: wakati_reading(x))\n",
    "data = data.drop(['text'], axis=1)\n",
    "    \n",
    "data.to_csv('./data/feature_data_jp_ja.csv', header=False, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def main():\n",
    "    data = pd.read_csv('./data/feature_data_jp_ja.csv', sep=',', names=['label', 'feature'])\n",
    "    \n",
    "    train = data.drop(['label'], axis=1)\n",
    "    train_label = data['label']\n",
    "    \n",
    "    train_X, val_X, train_Y, val_Y = train_test_split(train, train_label,\n",
    "                                                  test_size = .2,\n",
    "                                                  random_state=12)\n",
    "    \n",
    "    tf = TfidfVectorizer(analyzer='word', ngram_range=(1,6), max_features=10000,\n",
    "                    sublinear_tf=True, token_pattern=u'[A-Za-z0-9\\-ぁ-ヶ亜-黑ー]{1,}')\n",
    "\n",
    "    train_X_tf =  tf.fit_transform(train_X['feature'])\n",
    "    val_X_tf =  tf.transform(val_X['feature'])\n",
    "\n",
    "    clf_final = Pipeline([('tfidf', TfidfVectorizer(analyzer='word', ngram_range=(1,3), max_features=5000,\n",
    "                    sublinear_tf=True, token_pattern=u'[A-Za-z0-9\\-ぁ-ヶ亜-黑ー]{1,}')),\n",
    "                           ('clf', GradientBoostingClassifier(\n",
    "                                random_state = 1337,\n",
    "                                verbose = 0,\n",
    "                                n_estimators = 20,\n",
    "                                learning_rate = 0.1,\n",
    "                                loss = 'deviance',\n",
    "                                max_depth = 3\n",
    "                               ))])\n",
    "    \n",
    "    clf_final = clf_final.fit(train_X['feature'], train_Y)\n",
    "    \n",
    "    joblib.dump(clf_final, './model/ja_jp_v7.pkl')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
